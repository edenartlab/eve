name: Create
description: Generate or edit images and videos.
tip: |-
  This tool generates and edits both images and videos based on the output parameter.

  Rules:
  - Use 16:9, 1:1, and 9:16 aspect ratios for landscape, square, and portrait images respectively. Only use the other ones if making images (not videos) and it is specifically requested of you. If no aspect ratio is specified, use your best judgment depending on the content requested. Cinematic content should be landscape while social media content should be square or portrait.
  - For images: If the user wants to make an infographic, logo, or any other image that requires a lot of coherent and legible text, or the user requests OpenAI image tools, you should include "text_precision" in extras.
  - For images: If either "text_precision" is in extras *OR* there is an init_image, the prompt should be structured as an **INSTRUCTION**, unless there is a lora.
  - For images: If the user has an init_image but also has a lora or "controlnet" in extras, ignore the last rule about being an instruction and instead make a normal text-to-image prompt.
  - For videos: If you want to put specific lines of dialogue into the video, you must use "pro" quality with sound_effects, and no init_image).
  - For videos: Audio files are *only* supported when "talking_head" is in extras, so don't use one otherwise. If you want to mix an audio file over video, use the media_editor tool instead.

  Prompt guide:

  If you are generating an image from scratch or have a lora selected, use a normal text-to-image prompt (no instructions). May contain a primary subject and its action, including context like artistic medium, background, secondary items, color schemes, style or genre, mood, lighting, perspective, textures, time period, and cultural elements.

  If you are editing an image, structure your prompt as a single, explicit, imperative **Instruction** that references the input image(s) and fully specifies the outcome. Follow these guidelines:

  - For single-image edits, begin with “In this image, …”. Don't use "in this image"
  - If user requests to create a whole new image which only imports content or style from the reference image, don't use "in this image" and instead describe the new image in detail, only making references to what you want to keep from the input images.
  - For multiple image inputs, refer to images by index/role (e.g., “Image 1: subject,” “Image 2: background/logo”). Refer to things with consistent names.
  - Refer to objects by what they are rather than prior conversation names (e.g., “Turn the woman’s head into a green pumpkin,” not “Turn Jane’s head…”).
  - State precise edits/placements in concrete terms: what to add/change/remove and exactly where (e.g., “place the logo on the chest,” “three‑quarter view at ~30°,” “shoulder‑up crop,” “apply the style of image 2 onto the image of the woman's face”).
  - If rendering text, place quotation marks around the text.
  - Use positive modifiers rather than negatives (e.g. “empty street” rather than “no cars”)
      
  In either case, if the user gives vague or incomplete instructions to you, you may enhance their prompt to make it rich and detailed, but *always* strive to be as faithful to the user's intent as possible.

  For videos: Include details about the camera motion (if any), and action or dynamics, what's happening in the scene. Any details about the audio or dialogue should be excluded from here and instead specified in the sound_effects parameter.
  
cost_estimate: '(output == "video" ? (((quality == "pro" ? 40 : 20) + (sound_effects ? 5 : 0)) * duration) : (5 * n_samples))'
output_type: image
active: true
visible: true
handler: modal
parameters:
  output:
    type: string
    label: Output type
    description: Whether to generate an image or video
    default: "image"
    choices: ["image", "video"]
    required: true
  prompt:
    type: string
    label: Prompt
    description: Text prompt.
    tip: |-
      If generating from scratch, the prompt should be a visually detailed description of the image or video you want to generate. If using init or reference images, specify an instruction for how to edit or transform the input image(s) or create a new image with elements from the reference image(s).
    required: true
  aspect_ratio:
    type: string
    label: Aspect ratio
    description: Aspect ratio width/height
    default: "auto"
    choices: ["auto", "21:9", "16:9", "3:2", "4:3", "5:4", "1:1", "4:5", "3:4", "2:3", "9:16", "9:21"]
  init_image:
    type: image
    label: Input image
    description: Input image.
    tip: |-
      For images: Only use this if you are editing an image, generating an image based on a reference image, or using "controlnet" in extras.
      For videos: Use this as init_image if you are animating an image, i.e. using it as the first frame of the video. If generating a video from scratch, do not use this.
  seed:
    type: integer
    label: Seed
    description: Random seed for reproducible generation
    tip: Optional seed value for consistent results. If not provided, a random seed will be used.
    required: false
  extras:
    type: array
    label: Extra features
    description: Optional extra features to enable
    items:
      type: string
      enum: ["text_precision", "talking_head", "double_character", "controlnet"]
    tip: |-
      Choose any combination of extra features:
      - text_precision: For images only. Maximize the quality and legibility small details, especially text. Useful for infographics, signs, logos, thought bubbles, data visualizations, and other images that require a high degree of coherence, detail, and legibility, especially with text. *Not* compatible with LoRA! If you need to combine this with LoRA, it is better to first generate the initial image with the LoRA, then edit that image in a suqbsequent step with text_precision.
      - talking_head: For videos only. Special mode for outputting talking avatar or lipsyncing videos with Hedra. You must feature a face in the prompt or init_image, your audio must feature speech, and you want to make a video with the face speaking, singing, or lipsyncing.
      - double_character: For images only. Use this only when there are two LoRAs of characters/faces/people, or if the user requests it. It will attempt to make one image that generates both characters in it.
      - controlnet: You should almost NEVER use this. For images only. Conform the output image to the match the contours of the init_image. Useful for style transfer with strict content preservation. Generic image editing tasks should not use this.

    default: []
  end_image:
    type: image
    label: End image
    description: Last frame of video
    tip: |-
      For videos only. Use this if and only if you want to specify both the start and end image of the video. Requires both init_image and end_image. This tends not to work well. You should almost never use this unless the user requests it and gives you two images.
  duration:
    type: integer
    label: Duration
    description: Approximate duration of the video in seconds
    tip: For videos only.
    default: 10
    minimum: 5
    maximum: 10
  lora:
    type: lora
    label: LoRA model
    description: ID of LoRA model finetuned to specific style, face, or object.
  lora_strength:
    type: float
    label: LoRA strength
    description: Strength of the LoRA model
    tip: |-
      Higher values increase adherence/resemblance to the LoRA. Lower values may increase prompt adherence. Only works if you are using a LoRA. If the output does not look like the LoRA subject, can be increased (0.8-1.0). If the output does look like the subject but is ignoring the prompt too much, try decreating it (0.25-0.7).
    default: 0.8
    minimum: 0
    maximum: 1.2
  lora2:
    type: lora
    label: Second LoRA model
    description: ID of second LoRA finetuned model.
  lora2_strength:
    type: float
    label: Second LoRA strength
    description: Strength of the second LoRA
    tip: |-
      Same as lora_strength but for lora2.
    default: 0.8
    minimum: 0
    maximum: 1.2
  quality:
    type: string
    label: Quality
    description: Quality vs speed tradeoff
    tip: |-
      For videos only. "standard" is fast and cheap but lower quality. "pro" is slower and more expensive but highest quality. Use standard for most cases unless user specifically requests high quality.
    default: "standard"
    choices: ["standard", "pro"]
  model_preference:
    type: string
    label: Model preference
    description: Optional preference for specific model provider
    tip: |-
      Optionally specify which model provider to use. If not specified, will automatically choose the best model based on your requirements. For videos: kling, seedance, runway, veo. For images: seedream, openai, flux, nano_banana.
    choices: ["kling", "seedream", "seedance", "sdxl", "runway", "openai", "flux", "veo", "nano_banana"]
  audio:
    type: audio
    label: Audio track URL
    description: Optional audio track to mix with the video
    tip: |-
      Mix audio file into the video. For "talking_head", you must include this and it should be of speech only.
    required: false
  sound_effects:
    type: string
    label: Sound effects prompt
    description: Optional foley sound effects to mix over the video
    tip: |-
      For videos only. Only use this if you want to or have been requested to produce sound effects over the video. You should not use this if the user has already provided an audio track. You can include long and very detailed text about vocals, foley sound effects, backgroun effects, music, etc. You may include full lines of spoken dialogue, e.g. "the boy says 'I heard you can make dialogue in Eden now!'"
    required: false
  n_samples:
    type: integer
    label: Number of samples
    description: How many variations or samples to generate
    tip: For images only.
    default: 1
    minimum: 1
    maximum: 4
    required: false